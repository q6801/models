{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine\n",
    "\n",
    "* 이것은 뭐냐? 바로 선을 그어서 분류를 하는 방법이다.\n",
    "* 아니 그럼 logistic regression을 쓰면 되는거 아니냐?\n",
    "* ㄴㄴ 둘의 결정적인 차이가 있다. SVM은 최상의 마진(margin)을 찾으려고 시도하므로 데이터의 오류 위험이 줄어든다.\n",
    "* <strong>즉, svm은 대이터를 가르는 단순한 선이 아닌 직선과 데이터 사이의 마진을 고려한 모델이다.</strong>\n",
    "\n",
    "*** \n",
    "\n",
    "## logistic vs SVM\n",
    "1. SVM은 데이터의 기하학적 특성을 기반으로 만들어진 모델이고 로지스틱 회귀는 통계적 접근 방식을 기반으로한다. (그렇다고 한다)\n",
    "\n",
    "2. SVM은 텍스트 및 이미지와 같은 비정형 및 반정형 데이터와 잘 작동한다. 로지스틱은 이미 식별된 독립 변수와 함께 동작 (kernel을 사용하는 것을 의미하는듯)\n",
    "\n",
    "3. SVM은 overfitting의 위험이 상대적으로 덜하고 로지스틱은 더 취약하다.\n",
    "\n",
    "***\n",
    "### 사용 상황 구별\n",
    "n = feature의 수, m = train 데이터 개수\n",
    "\n",
    "\n",
    "1. n이 크고 (1-10000), m이 작은 경우(10-1000) : 선형 커널 + SVM, 로지스틱\n",
    "2. n이 작고 (1-1000), m이 중간인 경우 : 가우시안, polynomial 등 커널 + SVM\n",
    "3. n이 작고 (1-1000), m이 크면(50000-1000000+) : feature를 수동으로 추가하고 선형 커널 + svm, 로지스틱\n",
    "\n",
    "***\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![그림1](https://1.bp.blogspot.com/-n2tSWYD_XoA/WmwE1cP5ctI/AAAAAAAACfo/3eniT7SzJioB4SdJVL-HTJSb_ouTLLQ_gCK4BGAYYCw/s200/svm3.png)\n",
    "\n",
    "## 수식 유도 (MIT 강의)\n",
    "\n",
    "* 그래 위의 사진을 보니 경계(margin)을 갖는 svm이 뭔지 대충은 알겠네\n",
    "* 그래서 이걸 어떻게 만드는데???????????????????\n",
    "\n",
    "***\n",
    "### 초기 조건 (사진을 보세염)\n",
    "* 어떠한 직선 하나를 가정한다. (경계를 분간해주는 직선, 사진에선 점선)\n",
    "* 그 직선에 직교하는 벡터 : W\n",
    "* 어떠한 샘플 벡터 : u\n",
    "\n",
    "***\n",
    "\n",
    "* 어떠한 샘플 벡터 u가 +에 속하는지 -에 속하는지 아는게 우리의 목표이다..\n",
    "* 벡터 u와 직선에 직교하는 벡터 W를 내적한 후 그 값이 어떠한 상수보다 큰지 확인을 통해서 구별이 가능하다.\n",
    "* 일정한 상수보다 크면 +로 일정한 상수보다 작으면 -로 보낼거다\n",
    "![수식1](수식1.png)\n",
    "* svm은 단순한 직선에서 그치는 것이 아니라 margin을 찾는 작업이기 때문에 wx + b가 +일지 -일지를 1로 구별한다.\n",
    "* 즉, -1과 1사이면 구별의 boundary 사이에 있는 것이다.\n",
    "\n",
    "***\n",
    "\n",
    "* 이 두개로 이뤄진 식을 하나로 바꿀수는 없을까???????\n",
    "![수식2](수식2.png)\n",
    "***\n",
    "\n",
    "* 이 yi를 이용해서 수식을 하나로 변경이 가능하다.\n",
    "![수식3](../수식3.png)\n",
    "***\n",
    "\n",
    "* 그러면 boundary(경계선)에 정확히 걸칠때는 어떻게 표현할까요???\n",
    "![수식4](수식4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* 그렇다면 이 경계선의 길이는 어떻게 구할 수 있을까요?\n",
    "\n",
    "![ㅇ](https://2.bp.blogspot.com/-Kb58kvRn3p0/WmwKaCgfcQI/AAAAAAAACgA/FprZTNJDE7cLr_37_7AvnPuLe0PBuW9hgCK4BGAYYCw/s200/svm4.png)\n",
    "* 나란한 두 직선의 길이를 알기 위해서는 사진과 같이 x+ - x-를 한 벡터와 직선에 수직인 벡터 w만 있으면 구할 수 있다.\n",
    "\n",
    "* width = (x+ - x-)w/||w|| 즉, 두 점사이의 벡터와 수직인 단위벡터를 내적하면 길이가 나온다.\n",
    "\n",
    "\n",
    "* 위의 경계선에 걸칠때의 식인 yi(wxi + b) -1 = 0을 이용해서 x+ - x-를 구하면 2이므로 width = 2 / ||W|| = 2 / w벡터의 길이"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### optimization techniques\n",
    "* 위에서 얻은 식인 2 / ||W||는 두 경계선의 거리를 뜻하고 우리는 이를 최대화하는게 목표이다.\n",
    "![수식5](수식5.png)\n",
    "* 우리는 수학적 계산의 용이함을 위해서 이렇게 표현한다.\n",
    "\n",
    "* 위의 w를 이용한 수식을 만족해야하는 제약 조건을 갖고 있을 때 최적화 방법중 하나는 라그랑주 승수법을 적용할 수 있다.\n",
    "* 제약조건을 신경쓰지 않고 풀 수 있도록 문제를 바꿔준다. (으에??????)\n",
    "\n",
    "***\n",
    "![수식6](수식6.png)\n",
    "* 그러면 이렇게 변하고\n",
    "\n",
    "***\n",
    "![수식7](수식7.png)\n",
    "* 우리가 관심있는 각각의 변수에 대해 미분을 하면 다음의 식이 나온다.\n",
    "\n",
    "***\n",
    "![수식8](수식8.png)\n",
    "* 그걸 요로코롬 조로코롬 바꿔주면 짜잔 식이 나온다.\n",
    "***\n",
    "\n",
    "### 이 수식으로 얻는 결론\n",
    "* 전혀 모르겠는 식을 얻었다가 이번의 결론이 아니고 svm과 관련된 핵심 수식들이 모두 샘플에 대한 내적으로 이뤄졌다는 사실이다.\n",
    "\n",
    "\n",
    "* 내적이 왜 중요하냐면 실제로 두 샘플간의 거리를 가늠할 때 단순한 내적을 사용하는 것이 맞지 않는 경우가 상당히 있다.(선형이 아닌경우)\n",
    "\n",
    "* 그럴 때 현재 샘플의 공간을 어떤 함수를 디자인해서 샘플들이 선형으로 구별 가능한 다른 공간으로 이동하는 것을 사용한다는 것이다.\n",
    "\n",
    "\n",
    "* 엥 그런 방법이랑 내적이랑 뭔 상관이냐?\n",
    "* 바로 svm을 적용할 때 우리는 그 차원으로 보내는 함수가 아니라 그 공간에서의 내적값만 구할 수 있다면 non-linearly separable한 경우에도 잘 작동시킬 수 있다.\n",
    "\n",
    "\n",
    "* 이게 svm의 kernel trick이다.\n",
    "***\n",
    "\n",
    "## 참고\n",
    "http://jaejunyoo.blogspot.com/2018/01/support-vector-machine-1.html \n",
    "* 유튜브를 글로 정리\n",
    "\n",
    "\n",
    "https://www.youtube.com/watch?v=_PwhiWxHK8o&list=PLnvKubj2-I2LhIibS8TOGC42xsD3-liux&index=2&t=0s\n",
    "* 유튜브 강의\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cost function\n",
    "* 그래 위의 숄라 숄라한 방법으로 수식을 알아낸다라는 사실은 알겠어. 그래서 실제로 사용되는 cost function (loss function)이 뭔데?\n",
    "\n",
    "\n",
    "* 여기서 궁금한 점\n",
    "  *  위의 방식으로 svm을 구할 수 있다면 지금 같은 cost function은 어찌된거지? linear regression에서 선형 회귀를 구하는 식이 있지만 실제로는 gradient descent를 이용해서 값을 구하는 것과 동일한 이유인가????????????????????\n",
    "***\n",
    "* svm에서 사용하는 cost function은 hinge loss를 사용한다.\n",
    "* logistic regression의 cost function과 많이 유사하다.\n",
    "\n",
    "***\n",
    "![수식9](수식9.png)\n",
    "logistic regression의 cross-entropy\n",
    "***\n",
    "![수식10](수식10.png)\n",
    "support vector machine의 hinge-loss\n",
    "\n",
    "\n",
    "***\n",
    "\n",
    "## hinge-loss\n",
    "![수식11](수식11.png)\n",
    "* 간단하게 위의 그림같은 식이다.\n",
    "* 여기서 세타Tx는 위의 수식에서 구했던 yi(wx + b) > 1보다 클 때 +혹은 -를 성립하는 것을 생각하면 된다. 즉, 세타Tx를 wx + b로 생각하자\n",
    "\n",
    "\n",
    "* y = 1인 경우 : yi가 1일 때를 보면 wx + b가 1을 넘는 상황이 +예측과 실제로 +가 일치하는 경우고 반대로 1보다 작은 경우에는 그래프에서 보이듯이 패널티가 커지는 모습을 볼 수 있다.\n",
    "* y = 0인 경우 : yi가 -1일 때를 보면 wx + b가 -1보다 작은 상황이 -예측과 실제로 -가 일치하는 경우고 반대로 -1보다 큰 경우에는 그래프에서 보이듯이 패널티가 커지는 모습을 볼 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "* 다시 cross entropy를 한 번 살펴보면 어떠한 확률과 그 확률의 패널티(맞을때는 적고 틀리면 늘어나는, 여기서는 log(sigmoid(z)))로 구성되어있다.\n",
    "\n",
    "* 우리는 그 log(sigmoid(z))를 우리의 hinge-loss로 변경할거다.\n",
    "![수식12](수식12.png)\n",
    "***\n",
    "* 짜잔 우리는 cost function을 완성했다.\n",
    "* 그렇게 만든 우리의 식이 logistic과 얼마나 다른가 살펴보면\n",
    "<img src=\"https://i.imgur.com/F7NcV08.png\" width=\"300\" height=\"300\">\n",
    "\n",
    "* 거의 비슷하다.\n",
    "***\n",
    "* 하지만 차이를 비교하자면 hinge 로스는 값을 만족하는 데이터의 손실은 무시하지만 로지스틱 회귀에서는 이를 만족하더라도 손실이 0에 가까워지긴 하지만 완전히 0은 되지 않는다.\n",
    "* 사실 svm과 로지스틱 회귀의 손실함수가 비슷해서 그 학습결과 또한 유사한 경향을 보인다.\n",
    "\n",
    "## 참고\n",
    "https://www.slideshare.net/freepsw/svm-77055058\n",
    "* 아주 자세히 나옴. C와 세타 제곱을 식에 추가되어 있어서 ridge와 연계해서 생각해볼만한듯.\n",
    "\n",
    "https://towardsdatascience.com/optimization-loss-function-under-the-hood-part-iii-5dff33fa015d\n",
    "* 이것도 영어로 뭐라뭐라 함.\n",
    "\n",
    "https://ratsgo.github.io/machine%20learning/2017/10/12/terms/\n",
    "* hinge loss\n",
    "\n",
    "https://daeson.tistory.com/204\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 요한스 코드\n",
    "* 왜 미분을 통해서 gradient descent가 아닌 단순히 cost의 결과를 뺄까? 이게 맞는건가????????????????????????????????????????????????????????????????????????\n",
    "\n",
    "* 설명 너무 불친절 특히 세터와 관련된 정보가 없어. 세타 c, a, b 순서\n",
    "\n",
    "* 놀랍게도 cost function이란 함수로 세타를 변경시키는게 아닌 따로 gradient descent 함수 내에서 구함. 왜 이렇게 만든거야?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
